{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "267818ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468224a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text generation pipeline with model: gpt2...\n",
      "WARNING:tensorflow:From c:\\Users\\admin\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "--- Interactive Text Generation Mode ---\n",
      "Type your prompt and press Enter. Type 'quit' to exit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating text for prompt: 'artificial intellligence in healthcare'...\n",
      "Generation complete!\n",
      "\n",
      "--- Generated Text 1 ---\n",
      "artificial intellligence in healthcare: What does this mean for you?\n",
      "\n",
      "The study is published in the journal PLOS ONE.\n",
      "\n",
      "The authors say this is the first time they've looked at the risk of cancer in patients who are taking a pill. The study looked at patients with known or suspected cancer. The study looked at a range of health care providers from the US to the UK. The study looked at patients with known or suspected cancer.\n",
      "\n",
      "There are other indications that a large proportion of people who are taking a pill are taking a drug that is not properly regulated by the FDA.\n",
      "\n",
      "This could make it hard for patients to get the treatment they need.\n",
      "\n",
      "This is why the FDA says it has to develop a system that protects against side effects, which could mean taking a drug that's not regulated by the FDA.\n",
      "\n",
      "The FDA says that if a drug is not regulated by the FDA, it can be sold for as little as $60 in the US, or $100 in Canada.\n",
      "\n",
      "It's unclear how much the FDA could charge for a pill that is not regulated by the FDA.\n",
      "\n",
      "But it's unclear if the drug is effective.\n",
      "\n",
      "It's also unclear if it's safe to use.\n",
      "\n",
      "It's not known if\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "class TextGenerator:\n",
    "    \"\"\"\n",
    "    A simple text generation tool using Hugging Face's transformers library.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        \"\"\"\n",
    "        Initializes the text generator with a specified model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the pre-trained model to use from Hugging Face.\n",
    "                              Common choices: \"gpt2\", \"distilgpt2\", \"EleutherAI/gpt-neo-125M\",\n",
    "                                              \"EleutherAI/gpt-neo-1.3B\" (larger).\n",
    "        \"\"\"\n",
    "        print(f\"Loading text generation pipeline with model: {model_name}...\")\n",
    "        try:\n",
    "            self.generator = pipeline('text-generation', model=model_name)\n",
    "            print(\"Model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_name}: {e}\")\n",
    "            print(\"Please check if the model name is correct and you have an internet connection.\")\n",
    "            print(\"Falling back to 'gpt2' model (if it failed).\")\n",
    "            self.generator = pipeline('text-generation', model=\"gpt2\")\n",
    "\n",
    "        set_seed(42) # For reproducibility of results\n",
    "\n",
    "    def generate_text(self, prompt, max_length=100, num_return_sequences=1,\n",
    "                      do_sample=True, temperature=0.7, top_k=50, top_p=0.95):\n",
    "        \"\"\"\n",
    "        Generates text based on a given prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The initial text prompt to base the generation on.\n",
    "            max_length (int): The maximum total length of the generated text\n",
    "                              (including the prompt).\n",
    "            num_return_sequences (int): The number of independent sequences to generate.\n",
    "            do_sample (bool): If True, uses sampling for generation; otherwise, uses greedy decoding.\n",
    "                              Sampling is generally preferred for creative text.\n",
    "            temperature (float): Controls the randomness of predictions. Lower values\n",
    "                                 make the text more deterministic, higher values more random.\n",
    "                                 (Only applies if do_sample is True).\n",
    "            top_k (int): Filters out the k most likely next words.\n",
    "                         (Only applies if do_sample is True).\n",
    "            top_p (float): Filters out the smallest set of most likely next words\n",
    "                           whose cumulative probability exceeds p.\n",
    "                           (Only applies if do_sample is True).\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries, where each dictionary contains the generated text\n",
    "                  under the key 'generated_text'.\n",
    "        \"\"\"\n",
    "        if not prompt:\n",
    "            print(\"Warning: Empty prompt provided. Generating without a specific context.\")\n",
    "\n",
    "        print(f\"\\nGenerating text for prompt: '{prompt}'...\")\n",
    "        try:\n",
    "            generated_sequences = self.generator(\n",
    "                prompt,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "                do_sample=do_sample,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p\n",
    "            )\n",
    "            print(\"Generation complete!\")\n",
    "            return generated_sequences\n",
    "        except Exception as e:\n",
    "            print(f\"Error during text generation: {e}\")\n",
    "            return []\n",
    "\n",
    "# --- How to Use the Text Generator ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the generator\n",
    "    # You can change \"gpt2\" to other models like \"distilgpt2\" for faster generation\n",
    "    # or \"EleutherAI/gpt-neo-1.3B\" for potentially better quality (requires more RAM).\n",
    "    generator = TextGenerator(model_name=\"gpt2\")\n",
    "\n",
    "    # --- Interactive Mode (Loop) ---\n",
    "    print(\"\\n--- Interactive Text Generation Mode ---\")\n",
    "    print(\"Type your prompt and press Enter. Type 'quit' to exit.\")\n",
    "\n",
    "    while True:\n",
    "        user_prompt = input(\"\\nEnter your prompt: \")\n",
    "        if user_prompt.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        # Get generation parameters from user or use defaults\n",
    "        try:\n",
    "            max_len_input = input(f\"Max length (default: {100}): \")\n",
    "            max_length = int(max_len_input) if max_len_input else 100\n",
    "\n",
    "            num_seq_input = input(f\"Number of sequences (default: {1}): \")\n",
    "            num_return_sequences = int(num_seq_input) if num_seq_input else 1\n",
    "\n",
    "            do_sample_input = input(f\"Use sampling (y/n, default: y): \").lower()\n",
    "            do_sample = True if do_sample_input in ['y', 'yes', ''] else False\n",
    "\n",
    "            temperature = 0.7\n",
    "            top_k = 50\n",
    "            top_p = 0.95\n",
    "\n",
    "            if do_sample:\n",
    "                temp_input = input(f\"Temperature (0.1-2.0, default: {0.7}): \")\n",
    "                temperature = float(temp_input) if temp_input else 0.7\n",
    "                top_k_input = input(f\"Top-K (default: {50}): \")\n",
    "                top_k = int(top_k_input) if top_k_input else 50\n",
    "                top_p_input = input(f\"Top-P (default: {0.95}): \")\n",
    "                top_p = float(top_p_input) if top_p_input else 0.95\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Invalid input for generation parameters. Using defaults.\")\n",
    "            max_length = 100\n",
    "            num_return_sequences = 1\n",
    "            do_sample = True\n",
    "            temperature = 0.7\n",
    "            top_k = 50\n",
    "            top_p = 0.95\n",
    "\n",
    "        results = generator.generate_text(\n",
    "            user_prompt,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p\n",
    "        )\n",
    "\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"\\n--- Generated Text {i+1} ---\")\n",
    "            print(result['generated_text'])\n",
    "            print(\"-\" * (len(\"--- Generated Text ---\") + 4))\n",
    "\n",
    "    print(\"\\nExiting text generator. Goodbye!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
